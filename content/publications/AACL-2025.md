---
title: "Tackling Distractor Documents in Multi-Hop QA with Reinforcement and Curriculum Learning"
date: 2025-07-27
authors:
  [
    ["Jerry Huang", "https://jh603.github.io/"],
    ["Siddarth Madala", "/"],
    ["Risham Siddhu", "https://www.linkedin.com/in/risham-sidhu-b2a090150/"],
    ["Cheng Niu", "https://www.linkedin.com/in/cheng-niu-0b13712/"],
    ["Hao Peng", "https://haopeng-nlp.github.io/"],
    ["Julia Hockenmaier", "hmr-lab.github.io/"],
    ["Tong Zhang", "https://tongzhang-ml.org/"],
  ]
arxiv: "https://arxiv.org/abs/2503.12759"
conference: ["In submission at AACL 2025", "https://2025.aaclnet.org/"]

sitemap_exclude: True
---

Retrieval-augmented generation (RAG) systems rely on retrieval models for identifying relevant contexts and answer generation models for utilizing those contexts. However, retrievers exhibit imperfect recall and precision, limiting downstream performance. We introduce RAG-RL, an answer generation model trained for multi-hop question answering (MHQA) to not only generate answers but also to identify and cite relevant information from larger sets of retrieved contexts, shifting some of the burden of identifying relevant documents from the retriever to the answer generator. Our approach uses curriculum learning, where models are trained across retrieval settings with varying levels of noise. Our experiments show that training samples with fewer distractor documents enable models to acquire citation and reasoning skills with greater sample efficiency and generalizability, demonstrating strong model performance even as the number of irrelevant passages increases. We benchmark our methods on three open-domain MHQA datasets and report significant gains in answer and citation accuracy. Furthermore, our experiments provide empirical insights into how simpler training samples can give models stronger signals for learning specific skills (e.g., citation generation) and how different components of post-training (e.g., training set construction, rule-based rewards, training sample ordering, etc.) impact final model performance.
